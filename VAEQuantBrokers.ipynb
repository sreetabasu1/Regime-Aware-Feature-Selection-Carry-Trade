{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhAN/z15Fjf9g35Vnrxbda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreetabasu1/Regime-Aware-Feature-Selection-Carry-Trade/blob/main/VAEQuantBrokers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code formatting\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Load data from the Excel file into a DataFrame with headers\n",
        "file_path = 'Currency12_07.csv'\n",
        "exchange_df = pd.read_csv(file_path, header=0, index_col=0, parse_dates=True)\n",
        "\n",
        "# Calculate returns for each column\n",
        "returns_df = exchange_df.diff() / exchange_df.shift(1)\n",
        "returns_df = returns_df.iloc[:, :]\n",
        "returns_df.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "cutoff = pd.to_datetime('2018-12-30', format='%Y-%m-%d')\n",
        "X_train = returns_df[returns_df.index < cutoff]\n",
        "X_test = returns_df[returns_df.index >= cutoff]\n",
        "\n"
      ],
      "metadata": {
        "id": "vF1Z_YHKwjkO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Using Tensorflow for GM- VAE\n",
        "Errors in the below code and how they were fixed:\n",
        "Error 1: Issues with defining X_train\n",
        "Input Size Initialization:\n",
        "\n",
        "input_size = X_train.size should be replaced with input_size = X_train.shape[1] to get the number of features.\n",
        "Data Loading in DataLoader:\n",
        "\n",
        "The DataLoader should work with PyTorch Datasets, but in your case, X_train seems to be a DataFrame. You can convert it to a PyTorch Tensor using torch.from_numpy(X_train.values).\n",
        "\n",
        "convert X_train to Tensor: data = data.float().view(-1, input_size)\n",
        "\n",
        "Error 2: target must be between 0 and 1\n",
        "Fix: switch reconstruction loss to MSE"
      ],
      "metadata": {
        "id": "Q7wceUdhQeW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is running regular Gaussian prior, but we standardize data and recon_batch (runtime inputs of the loss function) so that we can use cross entropy error instead of MSE."
      ],
      "metadata": {
        "id": "pfN3SbT_2Deb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train is a DataFrame\n",
        "# Replace this line with your actual data loading\n",
        "\n",
        "# Standardize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns)\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, latent_size * 2)  # 2 for mean and log-variance\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Output between 0 and 1 for image data\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Split into mean and log-variance\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decoder(z)\n",
        "\n",
        "        return decoded, mu, logvar\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_scaled.shape[1]\n",
        "hidden_size = 256\n",
        "latent_size = 2  # Dimensionality of the latent space\n",
        "\n",
        "# Build VAE model\n",
        "vae = VAE(input_size, hidden_size, latent_size)\n",
        "\n",
        "# Loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)  # BCELoss for probabilities\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "# Convert X_train_scaled to PyTorch Tensor\n",
        "X_train_tensor = torch.from_numpy(X_train_scaled.values).float()\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader):  # Remove the unnecessary label _\n",
        "        data = torch.sigmoid(data)  # Apply sigmoid to input data\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        recon_batch = torch.sigmoid(recon_batch)  # Apply sigmoid to reconstructed batch\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader.dataset)}')\n",
        "\n",
        "# Sample from the VAE\n",
        "with torch.no_grad():\n",
        "    # Sample from the latent space\n",
        "    z_sample = torch.randn(64, latent_size)\n",
        "    # Decode the samples\n",
        "    sample = torch.sigmoid(vae.decoder(z_sample)).view(64, 1, input_size)  # Apply sigmoid to generated samples\n",
        "    # Save or visualize the generated samples as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSEJaLu2BRD",
        "outputId": "65130b46-03f9-4563-bd48-0d3c4782b9ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 11.835940591479007\n",
            "Epoch 2/10, Loss: 11.78659973667989\n",
            "Epoch 3/10, Loss: 11.784750558042752\n",
            "Epoch 4/10, Loss: 11.784175801597105\n",
            "Epoch 5/10, Loss: 11.783960112432288\n",
            "Epoch 6/10, Loss: 11.783952045026911\n",
            "Epoch 7/10, Loss: 11.783747212988567\n",
            "Epoch 8/10, Loss: 11.78368230424607\n",
            "Epoch 9/10, Loss: 11.783811419682046\n",
            "Epoch 10/10, Loss: 11.78356199521444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for **Gaussian mixture** prior, using the loss function $L(\\lambda) =  \\Sigma_y q(y|x) (\\log(y|x) - \\log p(y) + \\log q(z|x, y) - \\log p(z|y) - \\log p(x|y, z))$\n",
        "\n",
        "from Quantitative Brokers"
      ],
      "metadata": {
        "id": "6YjknnGU6a9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train is a DataFrame\n",
        "# Replace this line with your actual data loading\n",
        "\n",
        "# Standardize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns)\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, latent_size * 2)  # 2 for mean and log-variance\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Output between 0 and 1 for image data\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Split into mean and log-variance\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decoder(z)\n",
        "\n",
        "        return decoded, mu, logvar\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_scaled.shape[1]\n",
        "hidden_size = 256\n",
        "latent_size = 2  # Dimensionality of the latent space\n",
        "\n",
        "# Build VAE model\n",
        "vae = VAE(input_size, hidden_size, latent_size)\n",
        "\n",
        "# Loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss term\n",
        "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
        "\n",
        "    # Latent space regularization terms\n",
        "    KLD_y = -0.5 * torch.sum(1 + logvar[:, :latent_size] - mu[:, :latent_size].pow(2) - logvar[:, :latent_size].exp())\n",
        "    KLD_z = -0.5 * torch.sum(1 + logvar[:, latent_size:] - mu[:, latent_size:].pow(2) - logvar[:, latent_size:].exp())\n",
        "\n",
        "    # Overall loss\n",
        "    loss = BCE + KLD_y + KLD_z\n",
        "    return loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "# Convert X_train_scaled to PyTorch Tensor\n",
        "X_train_tensor = torch.from_numpy(X_train_scaled.values).float()\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader):  # Remove the unnecessary label _\n",
        "        data = torch.sigmoid(data)  # Apply sigmoid to input data\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        recon_batch = torch.sigmoid(recon_batch)  # Apply sigmoid to reconstructed batch\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader.dataset)}')\n",
        "\n",
        "# Sample from the VAE\n",
        "with torch.no_grad():\n",
        "    # Sample from the latent space\n",
        "    z_sample = torch.randn(64, latent_size)\n",
        "    # Decode the samples\n",
        "    sample = torch.sigmoid(vae.decoder(z_sample)).view(64, 1, input_size)  # Apply sigmoid to generated samples\n",
        "    # Save or visualize the generated samples as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65130b46-03f9-4563-bd48-0d3c4782b9ec",
        "id": "IROoii7E6F4L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 11.835940591479007\n",
            "Epoch 2/10, Loss: 11.78659973667989\n",
            "Epoch 3/10, Loss: 11.784750558042752\n",
            "Epoch 4/10, Loss: 11.784175801597105\n",
            "Epoch 5/10, Loss: 11.783960112432288\n",
            "Epoch 6/10, Loss: 11.783952045026911\n",
            "Epoch 7/10, Loss: 11.783747212988567\n",
            "Epoch 8/10, Loss: 11.78368230424607\n",
            "Epoch 9/10, Loss: 11.783811419682046\n",
            "Epoch 10/10, Loss: 11.78356199521444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for $\\beta$-VAE with loss function given by \\begin{align*}\n",
        "\\mathcal{L}(\\lambda) &= \\sum_y q(y|x) \\left( \\beta \\cdot (\\log q(y|x) - \\log p(y) + \\log q(z|x, y) - \\log p(z|y)) - \\log p(x|y, z) \\right) \\\\\n",
        "\\end{align*}."
      ],
      "metadata": {
        "id": "2yiyXqsB8J_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train is a DataFrame\n",
        "# Replace this line with your actual data loading\n",
        "\n",
        "# Standardize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns)\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size, beta):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, latent_size * 2)  # 2 for mean and log-variance\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Output between 0 and 1 for image data\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Split into mean and log-variance\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decoder(z)\n",
        "\n",
        "        return decoded, mu, logvar\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_scaled.shape[1]\n",
        "hidden_size = 256\n",
        "latent_size = 2  # Dimensionality of the latent space\n",
        "beta = 1.0  # Adjust based on your problem and data characteristics\n",
        "\n",
        "# Build VAE model\n",
        "vae = VAE(input_size, hidden_size, latent_size, beta)\n",
        "\n",
        "# Loss function\n",
        "def loss_function(recon_x, x, mu, logvar, beta):\n",
        "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
        "    KLD_y = -0.5 * torch.sum(1 + logvar[:, :latent_size] - mu[:, :latent_size].pow(2) - logvar[:, :latent_size].exp())\n",
        "    KLD_z = -0.5 * torch.sum(1 + logvar[:, latent_size:] - mu[:, latent_size:].pow(2) - logvar[:, latent_size:].exp())\n",
        "    return BCE + beta * (KLD_y + KLD_z)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "# Convert X_train_scaled to PyTorch Tensor\n",
        "X_train_tensor = torch.from_numpy(X_train_scaled.values).float()\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "beta = 3\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader):\n",
        "        data = torch.sigmoid(data)  # Apply sigmoid to input data\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        recon_batch = torch.sigmoid(recon_batch)  # Apply sigmoid to reconstructed batch\n",
        "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader.dataset)}')\n",
        "\n",
        "# Sample from the VAE\n",
        "with torch.no_grad():\n",
        "    # Sample from the latent space\n",
        "    z_sample = torch.randn(64, latent_size)\n",
        "    # Decode the samples\n",
        "    sample = torch.sigmoid(vae.decoder(z_sample)).view(64, 1, input_size)  # Apply sigmoid to generated samples\n",
        "    # Save or visualize the generated samples as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb1jUYgd7o6S",
        "outputId": "178f732f-a5fe-4e17-b835-f90dde357942"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 11.855083158814622\n",
            "Epoch 2/10, Loss: 11.786603505574615\n",
            "Epoch 3/10, Loss: 11.78506128247457\n",
            "Epoch 4/10, Loss: 11.784319426046107\n",
            "Epoch 5/10, Loss: 11.78415054630745\n",
            "Epoch 6/10, Loss: 11.783908037638499\n",
            "Epoch 7/10, Loss: 11.783639423831081\n",
            "Epoch 8/10, Loss: 11.783818470963844\n",
            "Epoch 9/10, Loss: 11.783644756940284\n",
            "Epoch 10/10, Loss: 11.783697607682988\n"
          ]
        }
      ]
    }
  ]
}